<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>卢文的博客</title>
    <link>http://luwen.tech/</link>
    <description>Recent content on 卢文的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    
	<atom:link href="http://luwen.tech/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>http://luwen.tech/post/k8s-aliyun-log/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://luwen.tech/post/k8s-aliyun-log/</guid>
      <description>Kubernetes集群中的日志收集 通用做法：    方案 优点 缺点     每个app的镜像中都集成日志收集组件 部署方便，kubernetes的yaml文件无须特别配置，可以为每个app自定义日志收集配置 强耦合，不方便应用和日志收集组件升级和维护且会导致镜像过大   单独创建一个日志收集组件跟app的容器一起运行在同一个pod中 低耦合，扩展性强，方便维护和升级 需要对kubernetes的yaml文件进行单独配置，略显繁琐   将所有的Pod的日志都挂载到宿主机上，每台主机上单独起一个日志收集Pod 完全解耦，性能最高，管理起来最方便 需要统一日志收集规则，目录和输出方式    阿里云日志服务： Logtail的DaemonSet： 目前打算基于阿里云的日志服务来做，主要优势有，资源节约，每个node起一个日志收集进程，不需要每个pod，也不需要统一日志收集规则，目录和输出方式，可以很方便的收集应用日志，目录，标准输出，错误输出
 测试的日志建议全部输出到标准输出或者标准错误输出
 运维简单、资源占用较少、支持采集容器标准输出以及容器文件、配置方式灵活
 一个Logtail需要采集该节点所有容器的日志，此种方式会存在一定的性能瓶颈，且各个业务日志之间的隔离性较弱
  基于VM或者物理机部署的应用，日志采集相关技术都比较完善，有比较健全的Logstash、Fluentd、FileBeats等。但在Docker中，尤其在k8s中，日志采集并没有很好的解决方案，主要原因如下：
 采集目标多：需要采集宿主机日志、容器内日志、容器stdout。针对每种数据源都有对应的采集软件，但缺乏一站式解决方案。 弹性伸缩难：k8s是一个分布式的集群，服务、环境的弹性伸缩对于日志采集带来了很大的困难，采集的动态性以及数据完整性是非常大的挑战。 运维成本大：现有的方案只能使用多种软件组合采集，各个软件组装起来的系统稳定性难以保障，且缺乏中心化的管理、配置、监控手段，运维负担巨大。 侵入性高：Docker Driver扩展需要修改底层引擎；一个Container对应一个采集Agent又会产生资源竞争和浪费。 采集性能低：正常情况下一个Docker Engine会运行数十个甚至数百个Container，此时开源Agent日志采集性能以及资源消耗十分堪忧。  对比常用的日志收集客户端
     logtail logstash fluentd     采集方式 宿主机文件 支持 支持 支持    container文件 支持自动发现 静态采集 静态采集    container stdout 支持自动发现 插件扩展 Docker driver   数据处理 处理方式 正则、anchor、分隔符、json任意组合 插件扩展 插件扩展    自动打标 支持 不支持k8s 不支持k8s    过滤 正则 插件扩展 插件扩展   配置 自动更新 支持 手动加载 支持    服务端配置 支持 Beta版本支持简单功能 辅助管理软件扩展   性能 采集性能 极简单核160M/s、正则20M/s 单核2M/s左右 单核3-5M/s    资源消耗 平均CPU 2%、内存 40M 10倍以上性能消耗 10倍以上性能消耗   可靠性 数据保存 支持 插件支持 插件支持    采集点位保存 所有均支持 只支持文件 插件支持   监控 本地监控 支持 支持 支持    服务端监控 支持 Beta版本支持简单功能 辅助监控软件扩展     机器组除支持静态ip设置外，也支持自定义标识的方式：所有Logtail只要定义了该标识则自动关联到对应的机器组。 一个Logtail可属于多个机器组，一个机器组可包含多个Logtail，实现Logtail与机器组的解耦。 一个采集配置可应用到多个机器组，一个机器组可关联多个采集配置，实现机器组与采集配置的解耦。  以上概念映射到k8s中，可实现各种灵活的配置：</description>
    </item>
    
    <item>
      <title></title>
      <link>http://luwen.tech/post/k8s-shijian/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://luwen.tech/post/k8s-shijian/</guid>
      <description>1. 存活和就绪的结果处理程序需要是互相独立的程序 如前所述，对于在Kubernetes上下文中部署的每个产品，应该实现2个分别处理HTTP请求“存活”和“就绪”的处理程序。这些探测器的处理程序需要独立实现自己的功能。 这适用于作业处理应用程序。对于Kubernetes，了解应用程序是否正在运行非常重要。如果存活/就绪逻辑被解耦了在新进程中运行，则结果不准确。
3. 不要在“存活”处理程序里实现任何逻辑。如果主线程正在运行，它需要返回状态200，如果不是，则返回5xx 这个探针让Kubernetes知道应用程序是正在运行还是停止运行。通过检查/.well-known/live
 在这个上下文中，“逻辑”意味着对互相连接的服务实施某种检查
 4. 在“就绪”探针的处理程序中实现逻辑，以便提供有关应用程序准备情况的详情 就绪探针让Kubernetes知道Pod是否已准备好接收HTTP请求。作为开发人员，在此处实现一些逻辑来检查应用程序的所有后端依赖的可用性非常重要。当实现就绪处理程序时，需要清楚的知道您的应用程序依赖于哪些功能。换句话说，在就绪处理程序里，需要运行所有步骤以保证应用程序已准备好接收和处理https请求，这非常重要。例如，如果应用程序需要建立与数据库的连接以准备处理HTTP请求，那么在“就绪”的处理程序中就必须检查是否已建立与数据库连接并能正常使用。 我并不建议实现任何让程序重新就绪的逻辑。这些逻辑可能会为系统中的某些组件带来危险。
5. 不要尝试在就绪处理程序上重新建立应用程序的就绪状态。这个探针只是为了检查应用程序是否准备就绪，而不是让应用程序就绪。 我并不建议实现任何让程序重新就绪的逻辑。这些逻辑可能会为系统中的某些组件带来危险。</description>
    </item>
    
  </channel>
</rss>